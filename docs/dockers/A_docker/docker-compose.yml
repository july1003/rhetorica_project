# Server A: docker-compose.yml
version: '3.8'

services:
  # 1. 메시지 브로커 (중앙 집중형)
  redis:
    image: redis:7.2-alpine
    container_name: central_redis
    ports:
      - "6379:6379"
    restart: always

  # 2. 메인 웹 서버 (FastAPI)
  web:
    build: .
    container_name: interview_api
    ports:
      - "8000:8000"
    env_file: .env
    depends_on:
      - redis

  # 3. 실시간 AI 워커 (STT, LLM 꼬리질문 전담)
  rt_worker:
    build: .
    container_name: worker_realtime
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    env_file: .env
    command: celery -A tasks worker -Q realtime_queue --concurrency=1 --loglevel=info

  # 4. LLM 서버 (Ollama 활용)
  ollama:
    image: ollama/ollama
    container_name: ollama_server
    ports:
      - "11434:11434"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]