면점자의 음성과 영상을 실시간으로 분석하려묜 미디어 스트림 서버가 필요해 현재  A,B,C 서버로 분리된 상태에서 웹서버는 A,  영상 분석 음성 분석은  B 서버가 담당하게 구성되어있어 미디어 스트림 서버 구성과 예제 코드를 알려줘

실시간 면접 시스템에서 음성과 영상을 분리하여 처리하는 구성은 성능 최적화를 위해 아주 좋은 전략입니다. 현재 A(FastAPI), B(AI 분석) 서버 구조에서 가장 효율적인 미디어 스트리밍 방식은 **WebRTC**를 활용하거나, 브라우저에서 **Binary 데이터(Chunk)**를 WebSocket으로 쏘아주는 방식입니다.

실시간성(Low Latency)이 핵심이므로, 구성 방법과 예제 코드를 정리해 드립니다.

---

## 1. 미디어 스트리밍 아키텍처 구성

현재 구조를 유지하면서 실시간 분석을 추가하려면 다음과 같이 설계하는 것이 효율적입니다.

1. **Server A (FastAPI):** 클라이언트와의 **WebSocket 엔드포인트** 역할을 하며, 들어오는 미디어 스트림 데이터를 **Redis Pub/Sub** 또는 **Message Queue**를 통해 서버 B로 전달합니다.
2. **Server B (Worker):** Redis에서 스트림 데이터를 구독(Subscribe)하여 즉시 분석(STT/Vision)을 수행하고 결과를 다시 전달합니다.

---

## 2. Server A: 미디어 스트림 수신 엔드포인트 (FastAPI)

클라이언트(면접자)로부터 바이너리 오디오/비디오 데이터를 실시간으로 받아 Redis로 전달하는 코드입니다.

**Python**

```
import os
import redis
from fastapi import FastAPI, WebSocket, WebSocketDisconnect

app = FastAPI()

# Redis 연결 (docker-compose의 central_redis 서비스 사용)
redis_client = redis.StrictRedis(host='central_redis', port=6379, db=0)

@app.websocket("/ws/stream/{interview_id}")
async def media_stream_endpoint(websocket: WebSocket, interview_id: str):
    await websocket.accept()
    print(f"인터뷰 시작: {interview_id}")
  
    try:
        while True:
            # 클라이언트로부터 바이너리 데이터(음성/영상 프레임) 수신
            data = await websocket.receive_bytes()
          
            # Redis 채널로 데이터 발행 (서버 B가 이 채널을 구독)
            redis_client.publish(f"stream:{interview_id}", data)
          
    except WebSocketDisconnect:
        print(f"인터뷰 종료: {interview_id}")
    except Exception as e:
        print(f"에러 발생: {e}")
```

---

## 3. Server B: 실시간 분석 워커 (Python/AI)

서버 A가 보낸 스트림을 실시간으로 읽어 STT나 영상 분석 모델에 전달하는 예제입니다.

**Python**

```
import redis
import numpy as np

def start_analysis_worker(interview_id: str):
    redis_client = redis.StrictRedis(host='central_redis', port=6379, db=0)
    pubsub = redis_client.pubsub()
    pubsub.subscribe(f"stream:{interview_id}")

    print(f"서버 B: {interview_id} 분석 루프 시작")

    for message in pubsub.listen():
        if message['type'] == 'message':
            frame_data = message['data']
          
            # 여기서 실시간 분석 수행
            # 1. 음성 분석 (STT) - 예: Whisper Streaming
            # text = stt_model.transcribe(frame_data)
          
            # 2. 영상 분석 - 예: OpenCV로 프레임 변환 후 감정 분석
            # frame = np.frombuffer(frame_data, dtype=np.uint8)
            # result = analyze_emotion(frame)
          
            # 분석 결과는 다시 Redis나 Phoenix로 로깅
            print(f"분석 중... 데이터 크기: {len(frame_data)} bytes")

if __name__ == "__main__":
    # 실제로는 인터뷰 세션별로 스레드나 프로세스를 생성하여 실행
    start_analysis_worker("test_session_001")
```

---

## 4. 운영 시 고려사항 (Z:\ 드라이브 활용)

* **임시 저장:** 실시간 분석과 별개로 전체 영상을 저장해야 한다면, 서버 A에서 수신한 청크(Chunk)를 `Z:\shared_data` (컨테이너 내 `/shared_data`)에 파일로 순차적으로 기록하세요.
* **성능 최적화:** `redis.publish` 방식은 구현이 쉽지만 데이터가 너무 크면 Redis 메모리에 부담이 됩니다. 영상 프레임이 클 경우 `/shared_data`에 파일을 쓰고 파일 경로만 Redis로 보낸 뒤, 서버 B에서 파일을 읽는 방식을 병행할 수 있습니다.
* **Arize Phoenix:** 서버 B에서 분석된 결과(STT 텍스트 등)를 Phoenix에 트레이스로 남기면, 질문과 답변 사이의 지연 시간(Latency)을 측정하기 매우 좋습니다.

---

**이 구성을 위해 서버 B의 Docker 환경에 추가적인 라이브러리(예: `redis`, `numpy`) 설치가 필요하신가요? 아니면 클라이언트(React 등)에서 스트림을 쏘아주는 프런트엔드 코드가 궁금하신가요?**
